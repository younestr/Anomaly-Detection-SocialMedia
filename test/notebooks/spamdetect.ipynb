{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Mention Count</th>\n",
       "      <th>Follower Count</th>\n",
       "      <th>Verified</th>\n",
       "      <th>Bot Label</th>\n",
       "      <th>Location</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132131</td>\n",
       "      <td>flong</td>\n",
       "      <td>Station activity person against natural majori...</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>2353</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Adkinston</td>\n",
       "      <td>2020-05-11 15:29:50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>289683</td>\n",
       "      <td>hinesstephanie</td>\n",
       "      <td>Authority research natural life material staff...</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>9617</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Sanderston</td>\n",
       "      <td>2022-11-26 05:18:10</td>\n",
       "      <td>both live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>779715</td>\n",
       "      <td>roberttran</td>\n",
       "      <td>Manage whose quickly especially foot none to g...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4363</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Harrisonfurt</td>\n",
       "      <td>2022-08-08 03:16:54</td>\n",
       "      <td>phone ahead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>696168</td>\n",
       "      <td>pmason</td>\n",
       "      <td>Just cover eight opportunity strong policy which.</td>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>2242</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Martinezberg</td>\n",
       "      <td>2021-08-14 22:27:05</td>\n",
       "      <td>ever quickly new I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704441</td>\n",
       "      <td>noah87</td>\n",
       "      <td>Animal sign six data good or.</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>8438</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Camachoville</td>\n",
       "      <td>2020-04-13 21:24:21</td>\n",
       "      <td>foreign mention</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User ID        Username                                              Tweet  \\\n",
       "0   132131           flong  Station activity person against natural majori...   \n",
       "1   289683  hinesstephanie  Authority research natural life material staff...   \n",
       "2   779715      roberttran  Manage whose quickly especially foot none to g...   \n",
       "3   696168          pmason  Just cover eight opportunity strong policy which.   \n",
       "4   704441          noah87                      Animal sign six data good or.   \n",
       "\n",
       "   Retweet Count  Mention Count  Follower Count  Verified  Bot Label  \\\n",
       "0             85              1            2353     False          1   \n",
       "1             55              5            9617      True          0   \n",
       "2              6              2            4363      True          0   \n",
       "3             54              5            2242      True          1   \n",
       "4             26              3            8438     False          1   \n",
       "\n",
       "       Location           Created At            Hashtags  \n",
       "0     Adkinston  2020-05-11 15:29:50                 NaN  \n",
       "1    Sanderston  2022-11-26 05:18:10           both live  \n",
       "2  Harrisonfurt  2022-08-08 03:16:54         phone ahead  \n",
       "3  Martinezberg  2021-08-14 22:27:05  ever quickly new I  \n",
       "4  Camachoville  2020-04-13 21:24:21     foreign mention  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\hp\\Downloads\\Anomaly-Detection-SocialMedia\\test\\data\\bot_detection_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User ID', 'Username', 'Tweet', 'Retweet Count', 'Mention Count',\n",
       "       'Follower Count', 'Verified', 'Bot Label', 'Location', 'Created At',\n",
       "       'Hashtags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting \n",
    "X = df['Tweet']\n",
    "y = df['Bot Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "# tokenization using BERTweet tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   562,  5178, 33896,    17, 48012,   856,    12,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tokenization with a sample text\n",
    "\n",
    "sample_text = \"Twitter bot detection is interesting!\"\n",
    "tokens = tokenizer(sample_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create Dataset Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "# Convert the labels to match the format\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = TweetDataset(train_encodings, y_train)\n",
    "test_dataset = TweetDataset(test_encodings, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the Pre-trained Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Training Arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Example training arguments (customize as needed)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory to save model checkpoints and outputs\n",
    "    evaluation_strategy=\"steps\",  # Perform evaluation at regular intervals\n",
    "    save_steps=500,  # Save model every 500 steps\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    "    logging_steps=100,  # Log every 100 steps\n",
    "    per_device_train_batch_size=8,  # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=8,  # Adjust based on your GPU memory\n",
    "    num_train_epochs=3,  # Total number of epochs\n",
    "    load_best_model_at_end=True,  # Automatically load the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initialize the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m----> 3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:367\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:4127\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4122\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(accelerator_kwargs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   4123\u001b[0m         \u001b[38;5;66;03m# Some values may need to go through non-accelerate aligned defaults\u001b[39;00m\n\u001b[0;32m   4124\u001b[0m         \u001b[38;5;66;03m# and we need to run the `__post_init__` to set them\u001b[39;00m\n\u001b[0;32m   4125\u001b[0m         accelerator_kwargs \u001b[38;5;241m=\u001b[39m AcceleratorConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maccelerator_kwargs)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m-> 4127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4130\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maccelerator_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4132\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[0;32m   4133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "\u001b[1;31mTypeError\u001b[0m: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
